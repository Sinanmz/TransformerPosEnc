{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iCIIwNxFsQaY",
        "outputId": "e07dd127-6e18-4d97-ecfa-25e785da5704"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import csv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/transformer')\n",
        "\n",
        "\n",
        "\n",
        "# Loading the training data\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/x_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    x_train = [list(map(int, row)) for row in reader]\n",
        "\n",
        "y_train = []\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/y_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        number = int(row[0])\n",
        "        y_train.append(number)\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/attention_mask_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    attention_mask_train = [list(map(int, row)) for row in reader]\n",
        "\n",
        "vocab_size = []\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/vocab_size.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        number = int(row[0])\n",
        "        vocab_size.append(number)\n",
        "\n",
        "torch.manual_seed(420)\n",
        "\n",
        "# Defining the hyperparameters:\n",
        "batch_size = 512\n",
        "block_size = 100\n",
        "learning_rate = 1e-5\n",
        "n_embed = 512\n",
        "n_vocab = vocab_size[0]\n",
        "head_count = 8\n",
        "n_layers = 8\n",
        "dropout = 0.2\n",
        "epochs = 10\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the training data into the device:\n",
        "x_train = torch.tensor(x_train, dtype=torch.long).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "attention_mask_train = torch.tensor(attention_mask_train, dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "0Xbc9I37sWik"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for i in range(len(x_train)):\n",
        "  train_data.append([x_train[i], attention_mask_train[i], y_train[i]])\n",
        "\n",
        "# Using DataLoader to divide training data into batches:\n",
        "data_loader_train = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "\n",
        "from models import DecoderPosEnc, EncoderPosEnc, DecoderPosEncWave, EncoderPosEncWave, Decoder, Encoder\n",
        "\n",
        "model = EncoderPosEncWave(n_vocab, n_embed, block_size, head_count, dropout, n_layers, device)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "print(sum(p.numel() for p in model.parameters())/1e6, 'M parameters')\n",
        "\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr= learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MFZY3tFHsWkb",
        "outputId": "608cf46b-b6a9-4b7b-e46e-20afe12e5282"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "31.146498 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN5EliO0sWmU",
        "outputId": "3dbb193f-837a-4a31-ac2a-d22bf055ed0f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Jul 23 17:55:29 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.105.17   Driver Version: 525.105.17   CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   64C    P0    30W /  70W |    805MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "# Training loop:\n",
        "for epoch in range(epochs):\n",
        "    batch_id = 0\n",
        "    for batch in tqdm(iter(data_loader_train), desc = f'Epoch {epoch}'):\n",
        "        # Saving checkpoints:\n",
        "        if batch_id % (len(data_loader_train)//2) == 0:\n",
        "            torch.save(model, f'/content/drive/MyDrive/transformer/checkpoints/EncoderPosEncWave/EncoderPosEncWave_{epoch}_{int(batch_id > 0)}.pt')\n",
        "        # Forward and backward pass:\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        x, attention_mask, y = batch\n",
        "        logits = model(x, attention_mask)\n",
        "        loss = F.cross_entropy(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        batch_id += 1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "byQxWKhCsWoM",
        "outputId": "dd7d5c91-64bc-40a7-c764-a07e19fca9a7"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 0: 100%|██████████| 132/132 [03:50<00:00,  1.75s/it]\n",
            "Epoch 1: 100%|██████████| 132/132 [03:47<00:00,  1.73s/it]\n",
            "Epoch 2: 100%|██████████| 132/132 [03:47<00:00,  1.72s/it]\n",
            "Epoch 3: 100%|██████████| 132/132 [03:47<00:00,  1.72s/it]\n",
            "Epoch 4: 100%|██████████| 132/132 [03:47<00:00,  1.73s/it]\n",
            "Epoch 5: 100%|██████████| 132/132 [03:47<00:00,  1.72s/it]\n",
            "Epoch 6: 100%|██████████| 132/132 [03:47<00:00,  1.73s/it]\n",
            "Epoch 7: 100%|██████████| 132/132 [03:47<00:00,  1.72s/it]\n",
            "Epoch 8: 100%|██████████| 132/132 [03:47<00:00,  1.73s/it]\n",
            "Epoch 9: 100%|██████████| 132/132 [03:47<00:00,  1.72s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the last checkpoint:\n",
        "torch.save(model, f'/content/drive/MyDrive/transformer/checkpoints/EncoderPosEncWave/EncoderPosEncWave_10_0.pt')"
      ],
      "metadata": {
        "id": "XbJBCf0LseZz"
      },
      "execution_count": 7,
      "outputs": []
    }
  ]
}