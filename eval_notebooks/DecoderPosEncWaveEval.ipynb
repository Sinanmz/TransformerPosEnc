{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Usb30pzfErAq",
        "outputId": "be01b5eb-5972-437b-8983-8facdea24aad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Device: cuda\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import csv\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/transformer')\n",
        "\n",
        "\n",
        "\n",
        "# Loading the training and test data\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/x_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    x_train = [list(map(int, row)) for row in reader]\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/x_test.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    x_test = [list(map(int, row)) for row in reader]\n",
        "\n",
        "y_train = []\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/y_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        number = int(row[0])\n",
        "        y_train.append(number)\n",
        "\n",
        "y_test = []\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/y_test.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    for row in reader:\n",
        "        number = int(row[0])\n",
        "        y_test.append(number)\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/attention_mask_train.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    attention_mask_train = [list(map(int, row)) for row in reader]\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/preprocessed_data/attention_mask_test.csv', 'r') as file:\n",
        "    reader = csv.reader(file)\n",
        "    attention_mask_test = [list(map(int, row)) for row in reader]\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f'Device: {device}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the training and test data into the device:\n",
        "x_train = torch.tensor(x_train, dtype=torch.long).to(device)\n",
        "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
        "attention_mask_train = torch.tensor(attention_mask_train, dtype=torch.long).to(device)\n",
        "\n",
        "x_test = torch.tensor(x_test, dtype=torch.long).to(device)\n",
        "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
        "attention_mask_test = torch.tensor(attention_mask_test, dtype=torch.long).to(device)"
      ],
      "metadata": {
        "id": "F3zsBTksEvjp"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = []\n",
        "for i in range(len(x_train)):\n",
        "  train_data.append([x_train[i], attention_mask_train[i], y_train[i]])\n",
        "\n",
        "test_data = []\n",
        "for i in range(len(x_test)):\n",
        "  test_data.append([x_test[i], attention_mask_test[i], y_test[i]])\n",
        "\n",
        "batch_size = 256\n",
        "\n",
        "# Using DataLoader to divide training and test data into batches:\n",
        "data_loader_train = DataLoader(train_data, batch_size = batch_size, shuffle = True)\n",
        "data_loader_test = DataLoader(test_data, batch_size = batch_size, shuffle = True)"
      ],
      "metadata": {
        "id": "CF3KeZjgEvny"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# Measuring training and test losses and accuracies of every checkpoint:\n",
        "train_losses  = []\n",
        "test_losses  = []\n",
        "train_accuracy = []\n",
        "test_accuracy = []\n",
        "\n",
        "for i in range(11):\n",
        "  for j in range(2):\n",
        "\n",
        "    if i == 10 and j == 1:\n",
        "        break\n",
        "\n",
        "    model = torch.load(f'/content/drive/MyDrive/transformer/checkpoints/DecoderPosEncWave/DecoderPosEncWave_{i}_{j}.pt')\n",
        "    model.eval()\n",
        "\n",
        "    train_losses_temp  = []\n",
        "    test_losses_temp  = []\n",
        "    sum_train = 0\n",
        "    sum_test = 0\n",
        "\n",
        "\n",
        "    for batch in tqdm(iter(data_loader_train), desc = f'Training data: Epoch {i}, Middle of Epoch: {bool(j)} '):\n",
        "      x, attention_mask, y = batch\n",
        "      logits = model(x, attention_mask)\n",
        "\n",
        "      results = torch.argmax(logits, dim=-1)\n",
        "      sum_train += torch.sum(results == y).item()\n",
        "\n",
        "      loss = F.cross_entropy(logits, y)\n",
        "      train_losses_temp.append(loss.item()*len(batch[0]))\n",
        "\n",
        "    train_losses.append(sum(train_losses_temp)/len(train_data))\n",
        "    train_accuracy.append(sum_train/len(train_data))\n",
        "\n",
        "    for batch in tqdm(iter(data_loader_test), desc = f'Test data:     Epoch {i}, Middle of Epoch: {bool(j)} '):\n",
        "      x, attention_mask, y = batch\n",
        "      logits = model(x, attention_mask)\n",
        "\n",
        "      results = torch.argmax(logits, dim=-1)\n",
        "      sum_test += torch.sum(results == y).item()\n",
        "\n",
        "      loss = F.cross_entropy(logits, y)\n",
        "      test_losses_temp.append(loss.item()*len(batch[0]))\n",
        "\n",
        "    test_losses.append(sum(test_losses_temp)/len(test_data))\n",
        "    test_accuracy.append(sum_test/len(test_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YE1ETIizEvpx",
        "outputId": "8336984e-f753-4c3e-b604-a9cbf1beb9b4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Training data: Epoch 0, Middle of Epoch: False : 100%|██████████| 264/264 [01:22<00:00,  3.20it/s]\n",
            "Test data:     Epoch 0, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.83it/s]\n",
            "Training data: Epoch 0, Middle of Epoch: True : 100%|██████████| 264/264 [01:23<00:00,  3.17it/s]\n",
            "Test data:     Epoch 0, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.48it/s]\n",
            "Training data: Epoch 1, Middle of Epoch: False : 100%|██████████| 264/264 [01:25<00:00,  3.07it/s]\n",
            "Test data:     Epoch 1, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.63it/s]\n",
            "Training data: Epoch 1, Middle of Epoch: True : 100%|██████████| 264/264 [01:25<00:00,  3.08it/s]\n",
            "Test data:     Epoch 1, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.72it/s]\n",
            "Training data: Epoch 2, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 2, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.63it/s]\n",
            "Training data: Epoch 2, Middle of Epoch: True : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 2, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.69it/s]\n",
            "Training data: Epoch 3, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 3, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.58it/s]\n",
            "Training data: Epoch 3, Middle of Epoch: True : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 3, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.68it/s]\n",
            "Training data: Epoch 4, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 4, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.63it/s]\n",
            "Training data: Epoch 4, Middle of Epoch: True : 100%|██████████| 264/264 [01:25<00:00,  3.08it/s]\n",
            "Test data:     Epoch 4, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.67it/s]\n",
            "Training data: Epoch 5, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 5, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.71it/s]\n",
            "Training data: Epoch 5, Middle of Epoch: True : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 5, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.66it/s]\n",
            "Training data: Epoch 6, Middle of Epoch: False : 100%|██████████| 264/264 [01:25<00:00,  3.08it/s]\n",
            "Test data:     Epoch 6, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.66it/s]\n",
            "Training data: Epoch 6, Middle of Epoch: True : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 6, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.72it/s]\n",
            "Training data: Epoch 7, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 7, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.66it/s]\n",
            "Training data: Epoch 7, Middle of Epoch: True : 100%|██████████| 264/264 [01:25<00:00,  3.07it/s]\n",
            "Test data:     Epoch 7, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.64it/s]\n",
            "Training data: Epoch 8, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 8, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.65it/s]\n",
            "Training data: Epoch 8, Middle of Epoch: True : 100%|██████████| 264/264 [01:25<00:00,  3.07it/s]\n",
            "Test data:     Epoch 8, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.65it/s]\n",
            "Training data: Epoch 9, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.07it/s]\n",
            "Test data:     Epoch 9, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.64it/s]\n",
            "Training data: Epoch 9, Middle of Epoch: True : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 9, Middle of Epoch: True : 100%|██████████| 4/4 [00:00<00:00,  4.58it/s]\n",
            "Training data: Epoch 10, Middle of Epoch: False : 100%|██████████| 264/264 [01:26<00:00,  3.06it/s]\n",
            "Test data:     Epoch 10, Middle of Epoch: False : 100%|██████████| 4/4 [00:00<00:00,  4.65it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving losses and accuracies:\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/Evaluation/DecoderPosEncWaveTrainLosses.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for number in train_losses:\n",
        "        writer.writerow([number])\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/Evaluation/DecoderPosEncWaveTestLosses.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for number in test_losses:\n",
        "        writer.writerow([number])\n",
        "\n",
        "with open('/content/drive/MyDrive/transformer/Evaluation/DecoderPosEncWaveTrainAccuracy.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for number in train_accuracy:\n",
        "        writer.writerow([number])\n",
        "with open('/content/drive/MyDrive/transformer/Evaluation/DecoderPosEncWaveTestAccuracy.csv', 'w', newline='') as file:\n",
        "    writer = csv.writer(file)\n",
        "    for number in test_accuracy:\n",
        "        writer.writerow([number])"
      ],
      "metadata": {
        "id": "ox1sia-FEvxh"
      },
      "execution_count": 5,
      "outputs": []
    }
  ]
}